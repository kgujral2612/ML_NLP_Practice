{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "39d7a482",
   "metadata": {},
   "source": [
    "# Sentiment Analysis using Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "6f9d3da6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     /Users/kaushambigujral/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/kaushambigujral/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/kaushambigujral/nltk_data...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import nltk\n",
    "nltk.download('omw-1.4')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "9d81afe7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>UserName</th>\n",
       "      <th>ScreenName</th>\n",
       "      <th>Location</th>\n",
       "      <th>TweetAt</th>\n",
       "      <th>OriginalTweet</th>\n",
       "      <th>Sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3799</td>\n",
       "      <td>48751</td>\n",
       "      <td>London</td>\n",
       "      <td>16-03-2020</td>\n",
       "      <td>@MeNyrbie @Phil_Gahan @Chrisitv https://t.co/i...</td>\n",
       "      <td>Neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3800</td>\n",
       "      <td>48752</td>\n",
       "      <td>UK</td>\n",
       "      <td>16-03-2020</td>\n",
       "      <td>advice Talk to your neighbours family to excha...</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3801</td>\n",
       "      <td>48753</td>\n",
       "      <td>Vagabonds</td>\n",
       "      <td>16-03-2020</td>\n",
       "      <td>Coronavirus Australia: Woolworths to give elde...</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3802</td>\n",
       "      <td>48754</td>\n",
       "      <td>NaN</td>\n",
       "      <td>16-03-2020</td>\n",
       "      <td>My food stock is not the only one which is emp...</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3803</td>\n",
       "      <td>48755</td>\n",
       "      <td>NaN</td>\n",
       "      <td>16-03-2020</td>\n",
       "      <td>Me, ready to go at supermarket during the #COV...</td>\n",
       "      <td>Extremely Negative</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   UserName  ScreenName   Location     TweetAt  \\\n",
       "0      3799       48751     London  16-03-2020   \n",
       "1      3800       48752         UK  16-03-2020   \n",
       "2      3801       48753  Vagabonds  16-03-2020   \n",
       "3      3802       48754        NaN  16-03-2020   \n",
       "4      3803       48755        NaN  16-03-2020   \n",
       "\n",
       "                                       OriginalTweet           Sentiment  \n",
       "0  @MeNyrbie @Phil_Gahan @Chrisitv https://t.co/i...             Neutral  \n",
       "1  advice Talk to your neighbours family to excha...            Positive  \n",
       "2  Coronavirus Australia: Woolworths to give elde...            Positive  \n",
       "3  My food stock is not the only one which is emp...            Positive  \n",
       "4  Me, ready to go at supermarket during the #COV...  Extremely Negative  "
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df = pd.read_csv ('Corona_NLP_train.csv', encoding= 'latin-1')\n",
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "4fc8add6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "82314"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df = train_df[['OriginalTweet', 'Sentiment']]\n",
    "train_df.size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "f2196bd4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7596"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df = pd.read_csv('Corona_NLP_test.csv', encoding = 'latin-1')\n",
    "test_df = test_df[['OriginalTweet', 'Sentiment']]\n",
    "test_df.size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "f8a7f296",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>OriginalTweet</th>\n",
       "      <th>Sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>TRENDING: New Yorkers encounter empty supermar...</td>\n",
       "      <td>Extremely Negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>When I couldn't find hand sanitizer at Fred Me...</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Find out how you can protect yourself and love...</td>\n",
       "      <td>Extremely Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>#Panic buying hits #NewYork City as anxious sh...</td>\n",
       "      <td>Negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>#toiletpaper #dunnypaper #coronavirus #coronav...</td>\n",
       "      <td>Neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3793</th>\n",
       "      <td>Meanwhile In A Supermarket in Israel -- People...</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3794</th>\n",
       "      <td>Did you panic buy a lot of non-perishable item...</td>\n",
       "      <td>Negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3795</th>\n",
       "      <td>Asst Prof of Economics @cconces was on @NBCPhi...</td>\n",
       "      <td>Neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3796</th>\n",
       "      <td>Gov need to do somethings instead of biar je r...</td>\n",
       "      <td>Extremely Negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3797</th>\n",
       "      <td>I and @ForestandPaper members are committed to...</td>\n",
       "      <td>Extremely Positive</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3798 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          OriginalTweet           Sentiment\n",
       "0     TRENDING: New Yorkers encounter empty supermar...  Extremely Negative\n",
       "1     When I couldn't find hand sanitizer at Fred Me...            Positive\n",
       "2     Find out how you can protect yourself and love...  Extremely Positive\n",
       "3     #Panic buying hits #NewYork City as anxious sh...            Negative\n",
       "4     #toiletpaper #dunnypaper #coronavirus #coronav...             Neutral\n",
       "...                                                 ...                 ...\n",
       "3793  Meanwhile In A Supermarket in Israel -- People...            Positive\n",
       "3794  Did you panic buy a lot of non-perishable item...            Negative\n",
       "3795  Asst Prof of Economics @cconces was on @NBCPhi...             Neutral\n",
       "3796  Gov need to do somethings instead of biar je r...  Extremely Negative\n",
       "3797  I and @ForestandPaper members are committed to...  Extremely Positive\n",
       "\n",
       "[3798 rows x 2 columns]"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b8d75e6",
   "metadata": {},
   "source": [
    "## Exploratory Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "396edf6c",
   "metadata": {},
   "source": [
    "Check for null values in the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "903dbdcf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False    41157\n",
      "Name: OriginalTweet, dtype: int64\n",
      "False    41157\n",
      "Name: Sentiment, dtype: int64\n",
      "\n",
      "False    3798\n",
      "Name: OriginalTweet, dtype: int64\n",
      "False    3798\n",
      "Name: Sentiment, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "for column in train_df.columns:\n",
    "    print(train_df[column].isnull().value_counts())\n",
    "print()  \n",
    "for column in test_df.columns:\n",
    "    print(test_df[column].isnull().value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4734dae1",
   "metadata": {},
   "source": [
    "Check for duplicate values in the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "fd4e25b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False    41157\n",
      "dtype: int64\n",
      "\n",
      "False    3798\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(train_df.duplicated().value_counts())\n",
    "print()\n",
    "print(test_df.duplicated().value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "20854d04",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRENDING: New Yorkers encounter empty supermarket shelves (pictured, Wegmans in Brooklyn), sold-out online grocers (FoodKick, MaxDelivery) as #coronavirus-fearing shoppers stock up https://t.co/Gr76pcrLWh https://t.co/ivMKMsqdT1\n",
      "\n",
      "When I couldn't find hand sanitizer at Fred Meyer, I turned to #Amazon. But $114.97 for a 2 pack of Purell??!!Check out how  #coronavirus concerns are driving up prices. https://t.co/ygbipBflMY\n",
      "\n",
      "Find out how you can protect yourself and loved ones from #coronavirus. ?\n",
      "\n"
     ]
    }
   ],
   "source": [
    "corpus = df.iloc[:, 4:5]\n",
    "for c in corpus['OriginalTweet'][:3]:\n",
    "    print(c)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ab4b60f",
   "metadata": {},
   "source": [
    "## Preprocessing\n",
    "\n",
    "- Removing links(https) and mentions(@)\n",
    "- Creating tokens\n",
    "- Stopword removal\n",
    "- Lemmatizing tokens\n",
    "- Un-tokenizing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "32472c67",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(text_df):\n",
    "    # converting to lowercase\n",
    "    text_df = text_df.apply(lambda x: x.lower())\n",
    "    #Removing links and mentions\n",
    "    text_df = text_df.apply(lambda x: re.sub(r\"((www.[^s]+)|(http\\S+)|(@\\S+))\", '', x))\n",
    "    #Tokenization\n",
    "    text_df = text_df.apply(lambda x: nltk.tokenize.RegexpTokenizer(r\"\\w+\").tokenize(x))\n",
    "    #Stopword removal\n",
    "    text_df = text_df.apply(lambda x: [i for i in x if i not in nltk.corpus.stopwords.words('english')])\n",
    "    #lemmatization\n",
    "    lm = nltk.WordNetLemmatizer()\n",
    "    text_df = text_df.apply(lambda x: [lm.lemmatize(i) for i in x])\n",
    "    print(text_df)\n",
    "    # Un- Tokenizing\n",
    "    text_df = text_df.apply(lambda x: ' '.join(x))\n",
    "    return text_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "f7512ca4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0       [trending, new, yorkers, encounter, empty, sup...\n",
      "1       [find, hand, sanitizer, fred, meyer, turned, a...\n",
      "2                [find, protect, loved, one, coronavirus]\n",
      "3       [panic, buying, hit, newyork, city, anxious, s...\n",
      "4       [toiletpaper, dunnypaper, coronavirus, coronav...\n",
      "                              ...                        \n",
      "3793    [meanwhile, supermarket, israel, people, dance...\n",
      "3794    [panic, buy, lot, non, perishable, item, echo,...\n",
      "3795    [asst, prof, economics, talking, recent, resea...\n",
      "3796    [gov, need, somethings, instead, biar, je, rak...\n",
      "3797    [member, committed, safety, employee, end, use...\n",
      "Name: OriginalTweet, Length: 3798, dtype: object\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0    trending new yorkers encounter empty supermark...\n",
       "1    find hand sanitizer fred meyer turned amazon 1...\n",
       "2                   find protect loved one coronavirus\n",
       "3    panic buying hit newyork city anxious shopper ...\n",
       "4    toiletpaper dunnypaper coronavirus coronavirus...\n",
       "Name: OriginalTweet, dtype: object"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preprocessed = preprocess(corpus['OriginalTweet'])\n",
    "preprocessed.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c775950d",
   "metadata": {},
   "source": [
    "## Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "5954f227",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(41157, 80424)"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "vectorizer = CountVectorizer()\n",
    "tweet_vector = vectorizer.fit_transform(train_df['OriginalTweet'])\n",
    "tweet_vector.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "c2000202",
   "metadata": {},
   "outputs": [],
   "source": [
    "#split the data into 80% training and 20% testing\n",
    "from sklearn.model_selection import train_test_split\n",
    "xtrain, xtest, ytrain, ytest = train_test_split(tweet_vector, train_df.Sentiment, test_size=0.20,random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "e839c39c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create and train the Naive Bayes Classifier\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "classifier = MultinomialNB().fit(xtrain, ytrain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "6a26db4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Extremely Positive' 'Positive' 'Negative' ... 'Neutral' 'Neutral'\n",
      " 'Positive']\n",
      "['Neutral' 'Negative' 'Positive' ... 'Neutral' 'Neutral' 'Positive']\n"
     ]
    }
   ],
   "source": [
    "print(classifier.predict(xtest))\n",
    "print(ytest.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "8f6f6d64",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                    precision    recall  f1-score   support\n",
      "\n",
      "Extremely Negative       0.96      0.54      0.69      4387\n",
      "Extremely Positive       0.92      0.64      0.76      5293\n",
      "          Negative       0.70      0.83      0.76      7931\n",
      "           Neutral       0.94      0.55      0.70      6187\n",
      "          Positive       0.62      0.93      0.75      9127\n",
      "\n",
      "          accuracy                           0.74     32925\n",
      "         macro avg       0.83      0.70      0.73     32925\n",
      "      weighted avg       0.79      0.74      0.73     32925\n",
      "\n",
      "\n",
      "Confusion Matrix: \n",
      " [[2380   17 1428   39  523]\n",
      " [   6 3407  207   20 1653]\n",
      " [  50   70 6589   82 1140]\n",
      " [  24   70  853 3410 1830]\n",
      " [  24  145  402   73 8483]]\n",
      "Accuracy: \n",
      " 0.7370994684889901\n"
     ]
    }
   ],
   "source": [
    "# Evaluating the model on the training data set\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "pred = classifier.predict(xtrain)\n",
    "print(classification_report(ytrain, pred))\n",
    "print()\n",
    "print(\"Confusion Matrix: \\n\", confusion_matrix(ytrain, pred))\n",
    "print(\"Accuracy: \\n\", accuracy_score(ytrain, pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "508bc56a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                    precision    recall  f1-score   support\n",
      "\n",
      "Extremely Negative       0.67      0.23      0.34      1094\n",
      "Extremely Positive       0.61      0.30      0.40      1331\n",
      "          Negative       0.42      0.52      0.46      1986\n",
      "           Neutral       0.68      0.33      0.44      1526\n",
      "          Positive       0.40      0.69      0.50      2295\n",
      "\n",
      "          accuracy                           0.46      8232\n",
      "         macro avg       0.55      0.41      0.43      8232\n",
      "      weighted avg       0.52      0.46      0.45      8232\n",
      "\n",
      "\n",
      "Confusion Matrix: \n",
      " [[ 252    7  655   13  167]\n",
      " [   3  402   85   15  826]\n",
      " [  83   35 1042   85  741]\n",
      " [  15   37  328  499  647]\n",
      " [  25  177  391  124 1578]]\n",
      "Accuracy: \n",
      " 0.4583333333333333\n"
     ]
    }
   ],
   "source": [
    "# Evaluating the model on the testing data set\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "pred = classifier.predict(xtest)\n",
    "print(classification_report(ytest, pred))\n",
    "print()\n",
    "print(\"Confusion Matrix: \\n\", confusion_matrix(ytest, pred))\n",
    "print(\"Accuracy: \\n\", accuracy_score(ytest, pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9c611c8",
   "metadata": {},
   "source": [
    "## Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "99556233",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3798, 80424)"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_tweet_vector = vectorizer.transform(test_df['OriginalTweet'])\n",
    "test_tweet_vector.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "413eb523",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Neutral' 'Positive' 'Extremely Positive' ... 'Neutral'\n",
      " 'Extremely Negative' 'Positive']\n",
      "0       Extremely Negative\n",
      "1                 Positive\n",
      "2       Extremely Positive\n",
      "3                 Negative\n",
      "4                  Neutral\n",
      "               ...        \n",
      "3793              Positive\n",
      "3794              Negative\n",
      "3795               Neutral\n",
      "3796    Extremely Negative\n",
      "3797    Extremely Positive\n",
      "Name: Sentiment, Length: 3798, dtype: object\n"
     ]
    }
   ],
   "source": [
    "pred = classifier.predict(test_tweet_vector)\n",
    "print(pred)\n",
    "print(test_df.Sentiment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "0422cdf8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                    precision    recall  f1-score   support\n",
      "\n",
      "Extremely Negative       0.72      0.13      0.22       592\n",
      "Extremely Positive       0.69      0.18      0.29       599\n",
      "          Negative       0.42      0.54      0.47      1041\n",
      "           Neutral       0.76      0.16      0.27       619\n",
      "          Positive       0.35      0.76      0.48       947\n",
      "\n",
      "          accuracy                           0.41      3798\n",
      "         macro avg       0.59      0.36      0.35      3798\n",
      "      weighted avg       0.55      0.41      0.37      3798\n",
      "\n",
      "\n",
      "Confusion Matrix: \n",
      " [[ 78   1 402   2 109]\n",
      " [  1 110  39   3 446]\n",
      " [ 25   8 563  17 428]\n",
      " [  2  10 161 102 344]\n",
      " [  2  30 187  11 717]]\n",
      "Accuracy: \n",
      " 0.4133754607688257\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(test_df.Sentiment, pred))\n",
    "print()\n",
    "print(\"Confusion Matrix: \\n\", confusion_matrix(test_df.Sentiment, pred))\n",
    "print(\"Accuracy: \\n\", accuracy_score(test_df.Sentiment, pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72fafc97",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
